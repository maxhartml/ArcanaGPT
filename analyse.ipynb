{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "analyse_log.ipynb\n",
    "\n",
    "Parses a training log file, then visualises the training/validation loss\n",
    "and HellaSwag accuracy over time, comparing against GPT-2/GPT-3 baselines.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For inline plots (Jupyter):\n",
    "%matplotlib inline\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1) Configure model size and known baseline metrics\n",
    "# ---------------------------------------------------------\n",
    "sz = \"124M\"  # e.g. 124M, 350M, 774M, 1558M\n",
    "\n",
    "loss_baseline = {\n",
    "    \"124M\": 3.2924,  # Validation loss for GPT-2 124M checkpoint\n",
    "}[sz]\n",
    "\n",
    "hella2_baseline = {  # HellaSwag baseline (GPT-2)\n",
    "    \"124M\": 0.294463,\n",
    "    \"350M\": 0.375224,\n",
    "    \"774M\": 0.431986,\n",
    "    \"1558M\": 0.488946,\n",
    "}[sz]\n",
    "\n",
    "hella3_baseline = {  # HellaSwag baseline (GPT-3)\n",
    "    \"124M\": 0.337,\n",
    "    \"350M\": 0.436,\n",
    "    \"774M\": 0.510,\n",
    "    \"1558M\": 0.547,\n",
    "}[sz]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2) Read and parse training log\n",
    "# ---------------------------------------------------------\n",
    "# Expecting lines of the form:\n",
    "#   step  stream  value\n",
    "# e.g. \"100 train 3.0123\"\n",
    "# where 'stream' âˆˆ {train, val, hella}\n",
    "\n",
    "log_filename = \"logs/log.txt\"\n",
    "\n",
    "with open(log_filename, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Group logs by stream in a dictionary, e.g. streams['train'][step] = loss\n",
    "streams = {}\n",
    "for line in lines:\n",
    "    step_str, stream_name, val_str = line.strip().split()\n",
    "    step = int(step_str)\n",
    "    val = float(val_str)\n",
    "    if stream_name not in streams:\n",
    "        streams[stream_name] = {}\n",
    "    streams[stream_name][step] = val\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3) Convert dictionaries to sorted (step, value) tuples\n",
    "# ---------------------------------------------------------\n",
    "streams_xy = {}\n",
    "for stream, data_dict in streams.items():\n",
    "    # Sort by step\n",
    "    sorted_items = sorted(data_dict.items())\n",
    "    # Unzip keys and values into separate lists\n",
    "    steps_list, values_list = zip(*sorted_items)\n",
    "    streams_xy[stream] = (np.array(steps_list), np.array(values_list))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4) Create figure with two subplots: Loss and HellaSwag\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# ============= Panel 1: Training & Validation Loss =============\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "# Plot training loss\n",
    "train_steps, train_vals = streams_xy[\"train\"]\n",
    "plt.plot(train_steps, train_vals, label=f'ArcanaGPT ({sz}) train loss')\n",
    "print(\"Min Train Loss:\", np.min(train_vals))\n",
    "\n",
    "# Plot validation loss\n",
    "val_steps, val_vals = streams_xy[\"val\"]\n",
    "plt.plot(val_steps, val_vals, label=f'ArcanaGPT ({sz}) val loss')\n",
    "print(\"Min Validation Loss:\", np.min(val_vals))\n",
    "\n",
    "# Optionally compare to GPT-2 baseline loss\n",
    "if loss_baseline is not None:\n",
    "    plt.axhline(y=loss_baseline, color='r', linestyle='--',\n",
    "                label=f\"OpenAI GPT-2 ({sz}) checkpoint val loss\")\n",
    "\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.yscale('log')  # Log scale for better visual separation\n",
    "plt.ylim(top=12.0)  # Adjust upper limit of y-axis\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# ============= Panel 2: HellaSwag Accuracy =============\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "hella_steps, hella_vals = streams_xy[\"hella\"]\n",
    "plt.plot(hella_steps, hella_vals, label=f\"ArcanaGPT ({sz})\")\n",
    "print(\"Max Hellaswag Accuracy:\", np.max(hella_vals))\n",
    "\n",
    "# Plot GPT-2 baseline accuracy\n",
    "if hella2_baseline is not None:\n",
    "    plt.axhline(y=hella2_baseline, color='r', linestyle='--',\n",
    "                label=f\"OpenAI GPT-2 ({sz}) checkpoint\")\n",
    "\n",
    "# Plot GPT-3 baseline accuracy\n",
    "if hella3_baseline is not None:\n",
    "    plt.axhline(y=hella3_baseline, color='g', linestyle='--',\n",
    "                label=f\"OpenAI GPT-3 ({sz}) checkpoint\")\n",
    "\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"HellaSwag Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
